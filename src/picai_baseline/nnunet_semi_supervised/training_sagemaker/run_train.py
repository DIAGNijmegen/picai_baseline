from sagemaker.local import LocalSession
from sagemaker.pytorch import PyTorch

sagemaker_session = LocalSession()
sagemaker_session.config = {'local': {'local_code': True}}

# To run SageMaker locally, you need a role that has access to AWS SageMaker and S3.
# You can call this role `SageMakerRole` (as below), or you can change the role name below.
# You can use the following command to configure your AWS CLI:
# aws configure

# Configure PyTorch (no training happens yet)
pytorch_estimator = PyTorch(
    entry_point='train.py',
    source_dir='code',
    role='SageMakerRole',
    instance_type='local',
    instance_count=1,
    volume_size=2500,  # in GB
    framework_version='1.8.0',
    py_version='py36',
    max_run=60*60*24*10,  # in seconds
    # checkpoint_s3_uri='s3://....',  # optional
    # checkpoint_local_path='/checkpoints/',
    hyperparameters={
        "workdir": "/tmp/",
    }
)

# Note: local storage (to be used during training, will be removed afterwards), is located at /tmp
# for SageMaker Training. For SageMaker Notebooks (classic) it's at /home/ec2-user/SageMaker/,
# for SageMaker Training it's at /tmp/, and for SageMaker inference it's at /.

# Train the estimator
pytorch_estimator.fit(
    inputs={
        'images': 's3://rumc-picai-d-training-public/images/',
        'labels': 's3://rumc-picai-d-training-public/picai_labels/',
        'scripts': 'file://code/',  # this will be replaced by the participant's code in an S3 bucket
    }
)

# Deploys the model that was generated by fit() to local endpoint in a container
# pytorch_predictor = pytorch_estimator.deploy(initial_instance_count=1, instance_type='local')

# Serializes data and makes a prediction request to the local endpoint
# response = pytorch_predictor.predict(data)

# Tears down the endpoint container and deletes the corresponding endpoint configuration
# pytorch_predictor.delete_endpoint()

# Deletes the model
# pytorch_predictor.delete_model()
